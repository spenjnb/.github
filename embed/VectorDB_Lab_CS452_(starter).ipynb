{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/spenjnb/.github/blob/main/embed/VectorDB_Lab_CS452_(starter).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4UOUNsUTsvcn"
      },
      "outputs": [],
      "source": [
        "# Download datasets from kaggle\n",
        "\n",
        "import json\n",
        "import os\n",
        "\n",
        "if not os.path.exists(\"lex-fridman-text-embedding-3-large-128.zip\"):\n",
        "  kaggle_json = {\"username\": \"michaeltreynolds\",\"key\": \"149701be742f30a8a0526762c61beea0\"}\n",
        "  kaggle_dir = os.path.join(os.path.expanduser(\"~\"), \".kaggle\")\n",
        "  os.makedirs(kaggle_dir, exist_ok=True)\n",
        "  kaggle_config_path = os.path.join(kaggle_dir, \"kaggle.json\")\n",
        "  with open(kaggle_config_path, 'w') as f:\n",
        "    json.dump(kaggle_json, f)\n",
        "\n",
        "  !kaggle datasets download -d michaeltreynolds/lex-fridman-text-embedding-3-large-128\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip kaggle data\n",
        "\n",
        "!unzip lex-fridman-text-embedding-3-large-128.zip\n",
        "!unzip lex-fridman-text-embedding-3-large-128/*.zip\n"
      ],
      "metadata": {
        "id": "h3swnD70x4FG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use specific libraries\n",
        "!pip install datasets==2.20.0 psycopg2==2.9.9 pgcopy==1.6.0\n",
        "import psycopg2"
      ],
      "metadata": {
        "id": "SYDFzWfv4HLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get your own trial account at timescaledb and paste your own connection string\n",
        "\n",
        "#TODO\n",
        "CONNECTION = \"postgres://tsdbadmin@vj0gmfiv9i.xs0ij8nr2v.tsdb.cloud.timescale.com:32627/tsdb?sslmode=require\""
      ],
      "metadata": {
        "id": "ukT4dY-z25XG"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use this if you want to start over on your postgres table!\n",
        "\n",
        "DROP_TABLE = \"DROP TABLE IF EXISTS podcast, segment\"\n",
        "with psycopg2.connect(CONNECTION) as conn:\n",
        "    cursor = conn.cursor()\n",
        "    cursor.execute(DROP_TABLE)\n",
        "    conn.commit() # Commit the changes\n"
      ],
      "metadata": {
        "id": "gpp_3EuU3SN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Useful function that takes a pd.DataFrame and copies it directly into a table.\n",
        "\n",
        "import pandas as pd\n",
        "import io\n",
        "import psycopg2\n",
        "\n",
        "from typing import List\n",
        "\n",
        "def fast_pg_insert(df: pd.DataFrame, connection: str, table_name: str, columns: List[str]) -> None:\n",
        "    \"\"\"\n",
        "        Inserts data from a pandas DataFrame into a PostgreSQL table using the COPY command for fast insertion.\n",
        "\n",
        "        Parameters:\n",
        "        df (pd.DataFrame): The DataFrame containing the data to be inserted.\n",
        "        connection (str): The connection string to the PostgreSQL database.\n",
        "        table_name (str): The name of the target table in the PostgreSQL database.\n",
        "        columns (List[str]): A list of column names in the target table that correspond to the DataFrame columns.\n",
        "\n",
        "        Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    conn = psycopg2.connect(connection)\n",
        "    _buffer = io.StringIO()\n",
        "    df.to_csv(_buffer, sep=\";\", index=False, header=False)\n",
        "    _buffer.seek(0)\n",
        "    with conn.cursor() as c:\n",
        "        c.copy_from(\n",
        "            file=_buffer,\n",
        "            table=table_name,\n",
        "            sep=\";\",\n",
        "            columns=columns,\n",
        "            null=''\n",
        "        )\n",
        "    conn.commit()\n",
        "    conn.close()"
      ],
      "metadata": {
        "id": "wZDxdvoP4Fov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Database Schema\n",
        "We will create a database with two tables: podcast and segment:\n",
        "\n",
        "**podcast**\n",
        "\n",
        "- PK: id\n",
        " - The unique podcast id found in the huggingface data (i,e., TRdL6ZzWBS0  is the ID for Jed Buchwald: Isaac Newton and the Philosophy of Science | Lex Fridman Podcast #214)\n",
        "- title\n",
        " - The title of podcast (ie., Jed Buchwald: Isaac Newton and the Philosophy of Science | Lex Fridman Podcast #214)\n",
        "\n",
        "**segment**\n",
        "\n",
        "- PK: id\n",
        " - the unique identifier for the podcast segment. This was created by concatenating the podcast idx and the segment index together (ie., \"0;1\") is the 0th podcast and the 1st segment\n",
        "This is present in the as the \"custom_id\" field in the `embedding.jsonl` and batch_request.jsonl files\n",
        "- start_time\n",
        " - The start timestamp of the segment\n",
        "- end_time\n",
        " - The end timestamp of the segment\n",
        "- content\n",
        " - The raw text transcription of the podcast\n",
        "- embedding\n",
        " - the 128 dimensional vector representation of the text\n",
        "- FK: podcast_id\n",
        " - foreign key to podcast.id"
      ],
      "metadata": {
        "id": "7Y2HkhMZmHFC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample document:\n",
        "# {\n",
        "#   \"custom_id\": \"89:115\",\n",
        "#   \"url\": \"/v1/embeddings\",\n",
        "#   \"method\": \"POST\",\n",
        "#   \"body\": {\n",
        "#     \"input\": \" have been possible without these approaches?\",\n",
        "#     \"model\": \"text-embedding-3-large\",\n",
        "#     \"dimensions\": 128,\n",
        "#     \"metadata\": {\n",
        "#       \"title\": \"Podcast: Boris Sofman: Waymo, Cozmo, Self-Driving Cars, and the Future of Robotics | Lex Fridman Podcast #241\",\n",
        "#       \"podcast_id\": \"U_AREIyd0Fc\",\n",
        "#       \"start_time\": 484.52,\n",
        "#       \"stop_time\": 487.08\n",
        "#     }\n",
        "#   }\n",
        "# }\n",
        "\n",
        "# Sample embedding:\n",
        "# {\n",
        "#   \"id\": \"batch_req_QZBmHS7FBiVABxcsGiDx2THJ\",\n",
        "#   \"custom_id\": \"89:115\",\n",
        "#   \"response\": {\n",
        "#     \"status_code\": 200,\n",
        "#     \"request_id\": \"7a55eba082c70aca9e7872d2b694f095\",\n",
        "#     \"body\": {\n",
        "#       \"object\": \"list\",\n",
        "#       \"data\": [\n",
        "#         {\n",
        "#           \"object\": \"embedding\",\n",
        "#           \"index\": 0,\n",
        "#           \"embedding\": [\n",
        "#             0.0035960325,\n",
        "#             126 more lines....\n",
        "#             -0.093248844\n",
        "#           ]\n",
        "#         }\n",
        "#       ],\n",
        "#       \"model\": \"text-embedding-3-large\",\n",
        "#       \"usage\": {\n",
        "#         \"prompt_tokens\": 7,\n",
        "#         \"total_tokens\": 7\n",
        "#       }\n",
        "#     }\n",
        "#   },\n",
        "#   \"error\": null\n",
        "# }"
      ],
      "metadata": {
        "id": "3EZuFdc9m9uP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create table statements that you'll write\n",
        "#TODO\n",
        "\n",
        "# may need to run this to enable vector data type if you didn't select AI in service\n",
        "CREATE_EXTENSION = \"CREATE EXTENSION IF NOT EXISTS vector;\"\n",
        "\n",
        "# TODO: Add create table statement\n",
        "CREATE_PODCAST_TABLE = \"\"\"\n",
        "CREATE TABLE IF NOT EXISTS podcast (\n",
        "      id    TEXT PRIMARY KEY,\n",
        "      title TEXT NOT NULL\n",
        "    );\n",
        "\"\"\"\n",
        "\n",
        "# TODO: Add create table statement\n",
        "CREATE_SEGMENT_TABLE = \"\"\"\n",
        "CREATE TABLE IF NOT EXISTS segment (\n",
        "      id         TEXT PRIMARY KEY,\n",
        "      podcast_id TEXT REFERENCES podcast(id),\n",
        "      start_time DOUBLE PRECISION,\n",
        "      end_time   DOUBLE PRECISION,\n",
        "      content    TEXT,\n",
        "      embedding  VECTOR(128)\n",
        "    );\n",
        "\"\"\"\n",
        "\n",
        "conn = psycopg2.connect(CONNECTION)\n",
        "# TODO: Create tables with psycopg2 (example: https://www.geeksforgeeks.org/executing-sql-query-with-psycopg2-in-python/)\n",
        "cur = conn.cursor()\n",
        "\n",
        "cur.execute(CREATE_EXTENSION)\n",
        "cur.execute(CREATE_PODCAST_TABLE)\n",
        "cur.execute(CREATE_SEGMENT_TABLE)\n",
        "\n",
        "conn.commit()\n",
        "conn.close()\n"
      ],
      "metadata": {
        "id": "bU6fFAwb5EYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Extract needed data out of JSONL files. This may be the hard part!\n",
        "\n",
        "# TODO: What data do we need?\n",
        "# TODO: What data is in the documents jsonl files?\n",
        "# TODO: What data is in the embedding jsonl files?\n",
        "# TODO: Get some pandas data frames for our two tables so we can copy the data in!\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "podcast_rows = []\n",
        "with open(\"batch_request.jsonl\", \"r\") as f:\n",
        "  for line in f:\n",
        "    rec = json.loads(line)\n",
        "    meta = rec[\"body\"][\"metadata\"]\n",
        "    podcast_rows.append({\n",
        "        \"id\": meta[\"podcast_id\"],\n",
        "        \"title\": meta[\"title\"]\n",
        "    })\n",
        "\n",
        "podcast_df = pd.DataFrame(podcast_rows)\n",
        "print(f\"Found {len(podcast_df)} podcasts\")\n",
        "fast_pg_insert(podcast_df, CONNECTION, \"podcast\", [\"id\", \"title\"])"
      ],
      "metadata": {
        "id": "v81052OY5BKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CHUNK_SIZE = 50_000\n",
        "buffer = []\n",
        "count = 0\n",
        "\n",
        "def flush():\n",
        "  global buffer\n",
        "  df = pd.DataFrame(buffer)\n",
        "  fast_pg_insert(\n",
        "      df, CONNECTION, \"segment\",\n",
        "      [\"id\",\"podcast_id\",\"start_time\",\"end_time\",\"content\",\"embedding\"]\n",
        "  )\n",
        "  buffer = []\n",
        "\n",
        "with open(\"embedding.jsonl\", \"r\") as fe, open(\"batch_request.jsonl\", \"r\") as fm:\n",
        "  meta_map = {}\n",
        "  for line in fm:\n",
        "    rec = json.loads(line)\n",
        "    cid = rec[\"custom_id\"]\n",
        "    m = rec[\"body\"][\"metadata\"]\n",
        "    meta_map[cid] = {\n",
        "        \"podcast_id\": m[\"podcast_id\"],\n",
        "        \"start_time\": m[\"start_time\"],\n",
        "        \"end_time\": m[\"end_time\"],\n",
        "        \"content\": rec[\"body\"][\"input\"]\n",
        "    }\n",
        "\n",
        "  for line in fe:\n",
        "    rec = json.loads(line)\n",
        "    cid = rec[\"custom_id\"]\n",
        "    emb = rec[\"response\"][\"body\"][\"data\"][0][\"embedding\"]\n",
        "    md = meta_map[cid]\n",
        "\n",
        "    buffer.append({\n",
        "        \"id\": cid,\n",
        "        \"podcast_id\": md[\"podcast_id\"],\n",
        "        \"start_time\": md[\"start_time\"],\n",
        "        \"end_time\": md[\"end_time\"],\n",
        "        \"content\": md[\"content\"],\n",
        "        \"embedding\": emb\n",
        "    })\n",
        "\n",
        "    count += 1\n",
        "    if count % CHUNK_SIZE == 0:\n",
        "      print(f\" -> flushing {count} rows\")\n",
        "      flush()\n",
        "\n",
        "if buffer:\n",
        "  print(f\" -> flushing final {len(buffer)} rows\")\n",
        "  flush()\n",
        "\n",
        "print(\"Done loading segments.\")"
      ],
      "metadata": {
        "id": "IHFGqFQ9zead"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### Optional #####\n",
        "# In addition to the embedding and document files you might like to load\n",
        "# the full podcast raw data via the hugging face datasets library\n",
        "\n",
        "# from datasets import load_dataset\n",
        "# ds = load_dataset(\"Whispering-GPT/lex-fridman-podcast\")\n"
      ],
      "metadata": {
        "id": "xo3Y8IHYRruE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO Copy all the \"podcast\" data into the podcast postgres table!\n"
      ],
      "metadata": {
        "id": "5W3f-2iTpGL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO Copy all the \"segment\" data into the segment postgres table!\n",
        "# HINT 1: use the recommender.utils.fast_pg_insert function to insert data into the database\n",
        "# otherwise inserting the 800k documents will take a very, very long time\n",
        "# HINT 2: if you don't want to use all your memory and crash\n",
        "# colab, you'll need to either send the data up in chunks\n",
        "# or write your own function for copying it up. Alternative to chunking maybe start\n",
        "# with writing it to a CSV and then copy it up?\n"
      ],
      "metadata": {
        "id": "ZTUsciGfpahF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## This script is used to query the database\n",
        "import os\n",
        "import psycopg2\n",
        "\n",
        "\n",
        "# Write your queries\n",
        "# Q1) What are the five most similar segments to segment \"267:476\"\n",
        "# Input: \"that if we were to meet alien life at some point\"\n",
        "# For each result return the podcast name, the segment id, segment raw text,  the start time, stop time, and embedding distance\n",
        "\n",
        "conn = psycopg2.connect(CONNECTION)\n",
        "cur = conn.cursor()\n",
        "cur.execute(\"\"\"\n",
        "\n",
        "\"\"\")\n",
        "for row in cur.fetchall():\n",
        "  print(row)\n",
        "\n",
        "conn.commit()\n",
        "conn.close()"
      ],
      "metadata": {
        "id": "NvkG-51G5IDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q2) What are the five most dissimilar segments to segment \"267:476\"\n",
        "# Input: \"that if we were to meet alien life at some point\"\n",
        "# For each result return the podcast name, the segment id, segment raw text, the start time, stop time, and embedding distance\n"
      ],
      "metadata": {
        "id": "Dq8ePSfrw8Ix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q3) What are the five most similar segments to segment '48:511'\n",
        "\n",
        "# Input: \"Is it is there something especially interesting and profound to you in terms of our current deep learning neural network, artificial neural network approaches and the whatever we do understand about the biological neural network.\"\n",
        "# For each result return the podcast name, the segment id, segment raw text,  the start time, stop time, and embedding distance\n"
      ],
      "metadata": {
        "id": "dmTK02bZk3pF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q4) What are the five most similar segments to segment '51:56'\n",
        "\n",
        "# Input: \"But what about like the fundamental physics of dark energy? Is there any understanding of what the heck it is?\"\n",
        "# For each result return the podcast name, the segment id, segment raw text,  the start time, stop time, and embedding distance\n"
      ],
      "metadata": {
        "id": "jcfhAKKQk9rV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q5) For each of the following podcast segments, find the five most similar podcast episodes. Hint: You can do this by averaging over the embedding vectors within a podcast episode.\n",
        "\n",
        "#     a) Segment \"267:476\"\n",
        "\n",
        "#     b) Segment '48:511'\n",
        "\n",
        "#     c) Segment '51:56'\n",
        "\n",
        "# For each result return the Podcast title and the embedding distance\n"
      ],
      "metadata": {
        "id": "OT4yTTn4k_iX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q6) For podcast episode id = VeH7qKZr0WI, find the five most similar podcast episodes. Hint: you can do a similar averaging procedure as Q5\n",
        "\n",
        "# Input Episode: \"Balaji Srinivasan: How to Fix Government, Twitter, Science, and the FDA | Lex Fridman Podcast #331\"\n",
        "# For each result return the Podcast title and the embedding distance\n"
      ],
      "metadata": {
        "id": "_oKIVjn4lBYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deliverables\n",
        "You will turn in a ZIP or PDF file containing all your code and a PDF file with the queries and results for questions 1-7."
      ],
      "metadata": {
        "id": "WBZVtZP4lDO2"
      }
    }
  ]
}